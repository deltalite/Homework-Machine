{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Machine Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are commonly used for the problem/field of question answering. As a type of neural network, transformers can do their job very well when given the right architecture and enough data. Below, we explore some different models using the `transformers` library. Created by Huggingface, you can get pretrained transformers very easily. \n",
    "\n",
    "If you want to go the not-so-easy route (and preferably have GPUs/TPUs), you might want to take a look at the short section below where I lay out steps you'd need to take to train an `ALBERT` model on the SQuAD dataset. Note that I didn't go this route because I didn't believe it was worth spending money or credits on compute and I didn't want my laptop running for days on end with potentially subpar results. \n",
    "\n",
    "The idea of training a model on my laptop was why I initially chose ALBERT to use for the Homework Machine - it's a lighter weight model compared to its better known relative, BERT. The below section explains how you would go about training ALBERT on the squad 2 dataset for those interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT Setup & Tuning\n",
    "\n",
    "To start off, we'll want to clone the [ALBERT repo](https://github.com/google-research/albert) and following the [fine-tuning on SQuAD](https://github.com/google-research/albert#fine-tuning-on-squad) instructions. The `pip` command below will do it for you. Just don't be like me and be sure to fill out the `...` where appropriate. \n",
    "\n",
    "That said, what should we replace the ellipses with? It wasn't super clear to me initially, so I added some comments below to help out anyone who wants to try out this training method. Some are file locations you want to create, others are from files you'll need to download from elsewhere. Let's take a look:\n",
    "\n",
    "```\n",
    "pip install -r albert/requirements.txt\n",
    "python -m albert.run_squad_v2 \\\n",
    "  --albert_config_file=... \\            # download an ALBERT model (ex. https://tfhub.dev/google/albert_large/3, \n",
    "                                        #   links in the readme of ALBERT) and within its folder, use \n",
    "                                        #   \\{your_model}\\assets\\albert_config.json \n",
    "                                        #   (your_model = albert_large_3 for the ex)\n",
    "                                        \n",
    "  --output_dir=... \\                    # output location (not sure what that output is at the moment)\n",
    "  \n",
    "  --train_file=... \\                    # for squad (v2), this is the location of train-v2.0.json \n",
    "                                        #   (downloaded from https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "                                        \n",
    "  --predict_file=... \\                  # same as above, except use dev-v2.0.json\n",
    "  \n",
    "  --train_feature_file=... \\            # a file made by TFRecordWriter. I called this file train_feature_file.tf\n",
    "  \n",
    "  --predict_feature_file=... \\          # assumed to be the same as above\n",
    "  \n",
    "  --predict_feature_left_file=... \\     # assumed to be the same as above\n",
    "  \n",
    "  --init_checkpoint=... \\               # as mentioned in the readme, can instead put:\n",
    "                                        #   --albert_hub_module_handle=https://tfhub.dev/google/albert_base/1\n",
    "                                        #   I have no checkpoints made so I'm fine with using their suggestion\n",
    "                                        \n",
    "  --spm_model_file=... \\                # from your downloaded model, something analagous to \n",
    "                                        #   \\albert_large_3\\assets\\30k-clean.model \n",
    "                                        #   (albert_large_3 is the model I'll be using)\n",
    "                                        \n",
    "  --do_lower_case \\\n",
    "  --max_seq_length=384 \\\n",
    "  --doc_stride=128 \\\n",
    "  --max_query_length=64 \\\n",
    "  --do_train \\\n",
    "  --do_predict \\\n",
    "  --train_batch_size=48 \\\n",
    "  --predict_batch_size=8 \\\n",
    "  --learning_rate=5e-5 \\\n",
    "  --num_train_epochs=2.0 \\\n",
    "  --warmup_proportion=.1 \\\n",
    "  --save_checkpoints_steps=5000 \\\n",
    "  --n_best_size=20 \\\n",
    "  --max_answer_length=30\n",
    "```\n",
    "There are a number of settings I didn't touch on - I'm sure the defaults are reasonable but feel free to look into different setting for things like batch size, epochs and learning rate if you want to make training faster or more manageable given your system's RAM. Definitely worth looking into if training time is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Library Question Answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutes all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, QuestionAnsweringPipeline\n",
    "# importing both torch and tf to see if it resolves model loading errors\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TFAutoModelForQuestionAnswering\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test out that the library works as desired. We'll be using the example for question answering taken from the [huggingface transformers site](https://huggingface.co/transformers/task_summary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ðŸ¤— Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "    outputs = model(inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    answer_start = tf.argmax(\n",
    "        answer_start_scores, axis=1\n",
    "    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = (\n",
    "        tf.argmax(answer_end_scores, axis=1) + 1\n",
    "    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface also has the super easy to use `pipeline` if you don't want a specific model. Below's a very quick demo using the context and questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Answer: 'over 32+'\n",
      "Question: What does ðŸ¤— Transformers provide?\n",
      "Answer: 'general-purpose\n",
      "architectures'\n",
      "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Answer: 'TensorFlow 2.0 and PyTorch'\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    result = nlp(question=question, context=text)\n",
    "    print(f\"Answer: '{result['answer']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, it works! Next we'll show an example of using another model not in the example. Find a model you want to use from the [models](https://huggingface.co/models) page and put its name into the `model_name` variable. \n",
    "\n",
    "I was a bit misled when first trying to use a model such as `deepset/xlm-roberta-large-squad2` when using the tensorflow AutoModelForQuestionAnswering - it showed a `404 can't load weights for...`. This wasn't true for all models, so I eventually though that it might be because the model was only compatible with PyTorch. As you can see below, using torch objects, the loading indeed works. It also brought to my attention that models have a PyTorch and/or TensorFlow tag on their models page (ex. [distilbert-base-uncased-distilled-squad](https://huggingface.co/distilbert-base-uncased-distilled-squad)). This hadn't been obvious to me, so I hope that this was written in the documentation somewhere and I missed it rather than this being a surprise error to users of the library.\n",
    "\n",
    "Once again, we'll take from the huggingface sample code (no more sample code after this, I promise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Answer: over 32+\n",
      "Question: What does ðŸ¤— Transformers provide?\n",
      "Answer: general-purpose architectures\n",
      "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2.0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
    "# model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Package\n",
    "\n",
    "As those familiar with SQuAD are probably aware, the dataset uses Wikipedia articles. Wikipedia has a huge amount of information, so why not use the `wikipedia` Python package to find the answers to questions our users pose? While there is a newer `Wikipedia-API` package created last year, I'm going to use the `wikipedia` package since it was released back in 2014 and has consequently will have its problems better fleshed out through Github issues and forum questions being raised in the past. It also has all the functionality I need, so that seems sufficient reason for me to use it.\n",
    "\n",
    "As we saw, with the `transformers` package, the actual question answering is very straightforward provided we have the \"context\". But a homework machine is supposed to make your life easy - why should you have to paste in the \"context\" where you think the answer is - at that point you'd already be able to find the answer yourself. So based on the question, the `wikipedia` package will be employed to search and find the context. This doesn't seem like a trivial task, but let's begin with a trivial example and see how this package works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
